{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ce3f84-5f60-49c7-847f-824a076981c1",
   "metadata": {},
   "source": [
    "# BGE M3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063fcabb-fdd3-416c-bad2-a9587e39feb1",
   "metadata": {},
   "source": [
    "## Dense embeddings only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee1f190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install FlagEmbedding\n",
    "# !pip install -U FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a6db9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Tuple\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, AutoConfig,\n",
    "    AutoTokenizer, PreTrainedTokenizer\n",
    ")\n",
    "\n",
    "from FlagEmbedding.abc.finetune.reranker import AbsRerankerRunner, AbsRerankerModel\n",
    "from FlagEmbedding.finetune.reranker.encoder_only.base.modeling import CrossEncoderModel\n",
    "from FlagEmbedding.finetune.reranker.encoder_only.base.trainer import EncoderOnlyRerankerTrainer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030eecb4-7be0-489f-8c77-75a03cf78142",
   "metadata": {},
   "outputs": [],
   "source": [
    "availible_models = [\"BAAI/bge-base-en-v1.5\", \"BAAI/bge-large-en-v1.5\", \"BAAI/bge-reranker-v2-m3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6060c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Model Arguments\n",
    "    \"model_name_or_path\": \"BAAI/bge-reranker-v2-m3\",  \n",
    "    \"config_name\": None,\n",
    "    \"tokenizer_name\": None,\n",
    "    \"cache_dir\": \"./cache\",\n",
    "    \"trust_remote_code\": False,\n",
    "    \"model_type\": \"encoder\",\n",
    "    \"token\": None,  # HF token \n",
    "    \n",
    "    # Data Arguments\n",
    "    \"train_data\": [\"./ft_data/training.json\"],  # Training path of the data\n",
    "    \"cache_path\": \"./data_cache\",\n",
    "    \"train_group_size\": 8,\n",
    "    \"query_max_len\": 32,\n",
    "    \"passage_max_len\": 128,\n",
    "    \"max_len\": 512,\n",
    "    \"pad_to_multiple_of\": None,\n",
    "    \"max_example_num_per_dataset\": 100000,\n",
    "    \"query_instruction_for_rerank\": \"Search query:\",  # Optional instruction\n",
    "    \"query_instruction_format\": \"{}{}\",\n",
    "    \"knowledge_distillation\": False,\n",
    "    \"passage_instruction_for_rerank\": \"Passage:\",  # Optional instruction\n",
    "    \"passage_instruction_format\": \"{}{}\",\n",
    "    \"shuffle_ratio\": 0.0,\n",
    "    \"sep_token\": \"\\n\",\n",
    "    \n",
    "    # Training Arguments\n",
    "    \"output_dir\": \"./results\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"max_steps\": 1000, \n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    # \"num_train_epochs\": 3,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"logging_dir\": \"./logs\",\n",
    "    \"logging_steps\": 25,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"fp16\": False,\n",
    "    \"sub_batch_size\": None,\n",
    "    \"report_to\": \"none\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3342d143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser\n",
    "\n",
    "from FlagEmbedding.abc.finetune.reranker import (\n",
    "    AbsRerankerModelArguments,\n",
    "    AbsRerankerDataArguments,\n",
    "    AbsRerankerTrainingArguments\n",
    ")\n",
    "from FlagEmbedding.finetune.reranker.encoder_only.base import EncoderOnlyRerankerRunner\n",
    "\n",
    "\n",
    "parser = HfArgumentParser((AbsRerankerModelArguments, AbsRerankerDataArguments, AbsRerankerTrainingArguments))\n",
    "\n",
    "model_args, data_args, training_args = parser.parse_dict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f217e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    PreTrainedTokenizer, \n",
    "    DataCollatorWithPadding,\n",
    "    BatchEncoding,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from typing import List\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class RerankerTrainDataset(Dataset):\n",
    "    \"\"\"Abstract class for reranker training dataset.\n",
    "\n",
    "    Args:\n",
    "        args (AbsRerankerDataArguments): Data arguments.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to use.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: AbsRerankerDataArguments,\n",
    "        tokenizer: PreTrainedTokenizer\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        train_datasets = []\n",
    "        for data_dir in args.train_data:\n",
    "            if not os.path.isdir(data_dir):\n",
    "                if not (data_dir.endswith('.json') or data_dir.endswith('.jsonl')): continue\n",
    "                temp_dataset = self._load_dataset(data_dir)\n",
    "                if len(temp_dataset) == 0: continue\n",
    "                train_datasets.append(temp_dataset)\n",
    "            else:\n",
    "                for file in os.listdir(data_dir):\n",
    "                    if not (file.endswith('.json') or file.endswith('.jsonl')): continue\n",
    "                    temp_dataset = self._load_dataset(os.path.join(data_dir, file))\n",
    "                    if len(temp_dataset) == 0: continue\n",
    "                    train_datasets.append(temp_dataset)\n",
    "\n",
    "\n",
    "        self.dataset = datasets.concatenate_datasets(train_datasets)\n",
    "\n",
    "        self.max_length = self.args.query_max_len + self.args.passage_max_len\n",
    "\n",
    "    # def _load_dataset(self, file_path: str):\n",
    "    #     \"\"\"Load dataset from path.\n",
    "\n",
    "    #     Args:\n",
    "    #         file_path (str): Path to load the datasets from.\n",
    "\n",
    "    #     Raises:\n",
    "    #         ValueError: `pos_scores` and `neg_scores` not found in the features of training data\n",
    "\n",
    "    #     Returns:\n",
    "    #         datasets.Dataset: Loaded HF dataset.\n",
    "    #     \"\"\"\n",
    "    #     if dist.get_rank() == 0:\n",
    "    #         logger.info(f'loading data from {file_path} ...')\n",
    "\n",
    "    #     temp_dataset = datasets.load_dataset('json', data_files=file_path, split='train', cache_dir=self.args.cache_path)\n",
    "    #     if len(temp_dataset) > self.args.max_example_num_per_dataset:\n",
    "    #         temp_dataset = temp_dataset.select(random.sample(list(range(len(temp_dataset))), self.args.max_example_num_per_dataset))\n",
    "    #     if not self.args.knowledge_distillation:\n",
    "    #         if 'pos_scores' in temp_dataset.column_names:\n",
    "    #             temp_dataset = temp_dataset.remove_columns(['pos_scores'])\n",
    "    #         if 'neg_scores' in temp_dataset.column_names:\n",
    "    #             temp_dataset = temp_dataset.remove_columns(['neg_scores'])\n",
    "    #     else:\n",
    "    #         if 'pos_scores' not in temp_dataset.column_names or 'neg_scores' not in temp_dataset.column_names:\n",
    "    #             raise ValueError(f\"`pos_scores` and `neg_scores` not found in the features of training data in {file_path}, which is necessary when using knowledge distillation.\")\n",
    "    #     return temp_dataset\n",
    "\n",
    "    def _load_dataset(self, file_path: str):\n",
    "        \"\"\"Load dataset from path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to load the datasets from.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: `pos_scores` and `neg_scores` not found in the features of training data\n",
    "\n",
    "        Returns:\n",
    "            datasets.Dataset: Loaded HF dataset.\n",
    "        \"\"\"\n",
    "        # Check if distributed is initialized before using it\n",
    "        is_main_process = True\n",
    "        if hasattr(dist, 'is_initialized') and dist.is_initialized():\n",
    "            is_main_process = dist.get_rank() == 0\n",
    "            \n",
    "        if is_main_process:\n",
    "            logger.info(f'loading data from {file_path} ...')\n",
    "\n",
    "        temp_dataset = datasets.load_dataset('json', data_files=file_path, split='train', cache_dir=self.args.cache_path)\n",
    "        if len(temp_dataset) > self.args.max_example_num_per_dataset:\n",
    "            temp_dataset = temp_dataset.select(random.sample(list(range(len(temp_dataset))), self.args.max_example_num_per_dataset))\n",
    "            \n",
    "        if not self.args.knowledge_distillation:\n",
    "            if 'pos_scores' in temp_dataset.column_names:\n",
    "                temp_dataset = temp_dataset.remove_columns(['pos_scores'])\n",
    "            if 'neg_scores' in temp_dataset.column_names:\n",
    "                temp_dataset = temp_dataset.remove_columns(['neg_scores'])\n",
    "        else:\n",
    "            if 'pos_scores' not in temp_dataset.column_names or 'neg_scores' not in temp_dataset.column_names:\n",
    "                raise ValueError(f\"`pos_scores` and `neg_scores` not found in the features of training data in {file_path}, which is necessary when using knowledge distillation.\")\n",
    "        return temp_dataset\n",
    "\n",
    "    def _shuffle_text(self, text):\n",
    "        \"\"\"shuffle the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Shuffled text.\n",
    "        \"\"\"\n",
    "        if self.args.shuffle_ratio > 0 and len(text) > 100 and random.random() < self.args.shuffle_ratio:\n",
    "            split_text = []\n",
    "            chunk_size = len(text)//3 + 1\n",
    "            for i in range(0, len(text), chunk_size):\n",
    "                split_text.append(text[i:i+chunk_size])\n",
    "            random.shuffle(split_text)\n",
    "            return \" \".join(split_text)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def create_one_example(self, qry_encoding: str, doc_encoding: str):\n",
    "        \"\"\"Creates a single input example by encoding and preparing a query and document pair for the model.\n",
    "\n",
    "        Args:\n",
    "            qry_encoding (str): Query to be encoded.\n",
    "            doc_encoding (str): Document to be encoded.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing tokenized and prepared inputs, ready for model consumption.\n",
    "        \"\"\"\n",
    "        qry_inputs = self.tokenizer.encode(qry_encoding, truncation=True, max_length=self.args.query_max_len + self.args.passage_max_len // 4, add_special_tokens=False)\n",
    "        doc_inputs = self.tokenizer.encode(doc_encoding, truncation=True, max_length=self.args.passage_max_len + self.args.query_max_len // 2, add_special_tokens=False)\n",
    "        item = self.tokenizer.prepare_for_model(\n",
    "            qry_inputs,\n",
    "            doc_inputs,\n",
    "            truncation='only_second',\n",
    "            max_length=self.args.query_max_len + self.args.passage_max_len,\n",
    "            padding=False,\n",
    "        )\n",
    "        return item\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = self.dataset[item]\n",
    "        train_group_size = self.args.train_group_size\n",
    "\n",
    "        query = data['query']\n",
    "        if self.args.query_instruction_for_rerank is not None:\n",
    "            query = self.args.query_instruction_format.format(\n",
    "                data['query_prompt'] if 'query_prompt' in data else self.args.query_instruction_for_rerank,\n",
    "                query\n",
    "            )\n",
    "\n",
    "        passages = []\n",
    "        teacher_scores = []\n",
    "\n",
    "        assert isinstance(data['pos'], list) and isinstance(data['neg'], list)\n",
    "\n",
    "        pos_idx = random.choice(list(range(len(data['pos']))))\n",
    "        passages.append(self._shuffle_text(data['pos'][pos_idx]))\n",
    "\n",
    "        neg_all_idx = list(range(len(data['neg'])))\n",
    "        if len(data['neg']) < train_group_size - 1:\n",
    "            num = math.ceil((train_group_size - 1) / len(data['neg']))\n",
    "            neg_idxs = random.sample(neg_all_idx * num, train_group_size - 1)\n",
    "        else:\n",
    "            neg_idxs = random.sample(neg_all_idx, self.args.train_group_size - 1)\n",
    "        for neg_idx in neg_idxs:\n",
    "            passages.append(data['neg'][neg_idx])\n",
    "\n",
    "        if self.args.knowledge_distillation:\n",
    "            assert isinstance(data['pos_scores'], list) and isinstance(data['neg_scores'], list)\n",
    "            teacher_scores.append(data['pos_scores'][pos_idx])\n",
    "            for neg_idx in neg_idxs:\n",
    "                teacher_scores.append(data['neg_scores'][neg_idx])\n",
    "            if not all(isinstance(score, (int, float)) for score in teacher_scores):\n",
    "                raise ValueError(f\"pos_score or neg_score must be digit\")\n",
    "        else:\n",
    "            teacher_scores = None\n",
    "\n",
    "        if self.args.passage_instruction_for_rerank is not None:\n",
    "            passages = [\n",
    "                self.args.passage_instruction_format.format(\n",
    "                    data['passage_prompt'] if 'passage_prompt' in data else self.args.passage_instruction_for_rerank, p\n",
    "                )\n",
    "                for p in passages\n",
    "            ]\n",
    "\n",
    "        batch_data = []\n",
    "        for passage in passages:\n",
    "            batch_data.append(self.create_one_example(query, passage))\n",
    "\n",
    "        return batch_data, teacher_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2edc2247-a2b9-4131-baec-278cfb72b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8505f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c55080cbd3483aab2476a2b20a811c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f1178539f4416e8166b44ba1e21fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc572238f51d4dc49c54a3dd3a7d1145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53124a16050446a0b3d3bc0e4d72814a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            token=model_args.token,\n",
    "            trust_remote_code=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76956c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813bc46a0a674e72b7a95e7a1033f097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/795 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_labels = 1\n",
    "\n",
    "config_model = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    token=model_args.token,\n",
    "    trust_remote_code=model_args.trust_remote_code,\n",
    ")\n",
    "# config_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b1ee550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab69c44727b48e4a23487468686629f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config_model,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    token=model_args.token,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    trust_remote_code=model_args.trust_remote_code\n",
    ")\n",
    "\n",
    "\n",
    "model = CrossEncoderModel(\n",
    "    base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_batch_size=training_args.per_device_train_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238d60b4-a02a-46a6-b606-f7b1c3356a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model\n",
    "# model.compute_loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89947666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.gradient_checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3efc025",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.gradient_checkpointing:\n",
    "    model.enable_input_require_grads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb965f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61e60083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args, data_args, training_args = parser.parse_dict(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85f00400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97bf61559df4bf58cc5527017d4c466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = RerankerTrainDataset(data_args, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ad83a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8,\n",
       " {'input_ids': [0, 33086, 41, 1294, 12, 52231, 621, 70, 28206, 2481, 112474, 100, 70, 33734, 25, 7, 17262, 9, 46799, 136, 43606, 9, 46799, 52831, 6496, 4620, 22299, 135094, 32, 2, 2, 57212, 429, 12, 3957, 33734, 1556, 40368, 52831, 6496, 4620, 22299, 135094, 26719, 10, 3650, 910, 5, 2389, 96090, 43606, 9, 46799, 178290, 28206, 214, 23, 14487, 387, 3882, 136, 10, 3650, 910, 5, 2389, 96090, 17262, 9, 46799, 178290, 28206, 214, 23, 14487, 387, 4046, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[10][0]), train_dataset[10][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b39ddd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RerankerCollator(DataCollatorWithPadding):\n",
    "    \"\"\"\n",
    "    The abstract reranker collator.\n",
    "    \"\"\"\n",
    "    query_max_len: int = config[\"query_max_len\"]\n",
    "    passage_max_len: int = config[\"passage_max_len\"]\n",
    "\n",
    "    def __call__(self, features) -> List[BatchEncoding]:\n",
    "        teacher_scores = [f[1] for f in features]\n",
    "        if teacher_scores[0] is None:\n",
    "            teacher_scores = None\n",
    "        elif isinstance(teacher_scores[0], list):\n",
    "            teacher_scores = sum(teacher_scores, [])\n",
    "\n",
    "        features = [f[0] for f in features]\n",
    "        if isinstance(features[0], list):\n",
    "            features = sum(features, [])\n",
    "\n",
    "        collated = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.query_max_len + self.passage_max_len,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pair\": collated,\n",
    "            \"teacher_scores\": teacher_scores,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e1db3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = RerankerCollator(query_max_len=32, passage_max_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edc3033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = RerankerCollator(\n",
    "            tokenizer= tokenizer,\n",
    "            query_max_len= config[\"query_max_len\"],\n",
    "            passage_max_len= config[\"passage_max_len\"],\n",
    "            pad_to_multiple_of=None,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "333552aa-5420-4c1d-a5f5-f0727da08b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RerankerTrainDataset at 0x7dc7935b4d90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a61a8a02-82b6-4da1-af3f-4055d747d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'dataset' is your dataset and 'collator' is your RerankerCollator instance\n",
    "dataloader = DataLoader(train_dataset, batch_size=1, collate_fn=data_collator)\n",
    "\n",
    "# Retrieve the first batch\n",
    "for batch in dataloader:\n",
    "    break  # This will give you the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3b9657a-1a20-41c5-b265-0de272cb3b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pair': {'input_ids': tensor([[     0,  33086,     41,   1294,     12,  52231,    509,     70, 169209,\n",
       "          227204,     23, 129264,  84458,    100,     70,    345,      5,    294,\n",
       "               5,  16839,      7,   1295,  72392,     47, 140429,     32,      2,\n",
       "               2,  57212,    429,     12,   3957, 129264,  84458,    100,     70,\n",
       "             345,      5,    294,      5,  16839,      7, 227204,     71,    390,\n",
       "             787,  11587,   1295,  72392,     47, 140429,      4,    678, 142424,\n",
       "          151134,   1295,   3650,  14604, 136473,     47,   3650, 196819,  45955,\n",
       "               2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,  33086,     41,   1294,     12,  52231,    509,     70, 169209,\n",
       "          227204,     23, 129264,  84458,    100,     70,    345,      5,    294,\n",
       "               5,  16839,      7,   1295,  72392,     47, 140429,     32,      2,\n",
       "               2,  57212,    429,     12,   4153,   5726,      4,     70,  14380,\n",
       "           11435,     71,     10, 131703,   7143,     23,  69919, 191618,     10,\n",
       "           21635,  31667, 240713,    450,    442,   1556,    110,  17808,   1363,\n",
       "              47,  47173,      9,  15982,    195,    100,   2499,  11015,      4,\n",
       "           13379,      4,    707,  22690,    400,  27519,   2449,    707,  14700,\n",
       "           47754,     23,  94878,    678,   2499, 140526,    707, 131703,   7143,\n",
       "               7,  26548,  47173,      9,  15982,    195,      5,      2],\n",
       "         [     0,  33086,     41,   1294,     12,  52231,    509,     70, 169209,\n",
       "          227204,     23, 129264,  84458,    100,     70,    345,      5,    294,\n",
       "               5,  16839,      7,   1295,  72392,     47, 140429,     32,      2,\n",
       "               2,  57212,    429,     12,    618,   2414,  29178,    159,      5,\n",
       "            1413,    720,  33284,    297, 194826,     23,    505,    237,  49952,\n",
       "           48180,  34202,      4,   9082,   1311,    309,   1428,    136, 196212,\n",
       "             136, 100512, 130922,  48180,  34202,      4,   9082,   1311,    309,\n",
       "            1428,    136, 196212,     23,  22482,   4152,      2,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,  33086,     41,   1294,     12,  52231,    509,     70, 169209,\n",
       "          227204,     23, 129264,  84458,    100,     70,    345,      5,    294,\n",
       "               5,  16839,      7,   1295,  72392,     47, 140429,     32,      2,\n",
       "               2,  57212,    429,     12,   3957,  51371,   3674,  53477,  63805,\n",
       "               7,    136,   1030, 146365,    214,  73048, 145870,     23,  16995,\n",
       "            7610,      4,  78426,    423,    132,     11,     16,  27750,    111,\n",
       "             903, 121129,    289,  34798,     98,  15236,  12417,    605,    621,\n",
       "           99201, 234311,     23,    903, 121129,    289,  34798,     98,  15236,\n",
       "           12417,    605,      5,      2,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,  33086,     41,   1294,     12,  52231,    509,     70, 169209,\n",
       "          227204,     23, 129264,  84458,    100,     70,    345,      5,    294,\n",
       "               5,  16839,      7,   1295,  72392,     47, 140429,     32,      2,\n",
       "               2,  57212,    429,     12,  47677,  64101,   1352,  37838,    201,\n",
       "           12807,    111,   2446,  93215,    621,  33636,    297,    390,     70,\n",
       "           87398,      7,  27985,   1314,    111,  21629,     15,    441,  11217,\n",
       "             247,     70,   8357,  67921,  44462,    111,  89917,    289,  27985,\n",
       "            1314,     15,  29877, 111351,     16,    707,   3789,  69941,      7,\n",
       "               5,      2,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,  33086,     41,   1294,     12,  52231,    509,     70, 169209,\n",
       "          227204,     23, 129264,  84458,    100,     70,    345,      5,    294,\n",
       "               5,  16839,      7,   1295,  72392,     47, 140429,     32,      2,\n",
       "               2,  57212,    429,     12,   1456,    979,    133,   2446,   1810,\n",
       "               9,   4390,      9,   6276,  29178,  33120,  81997, 121297,    621,\n",
       "              98,  83080,  77546,   3501,     23,      9,   1179,  18244,  60091,\n",
       "           18264,  81997, 121297,      4,    642,    765,   7228,  79825,     47,\n",
       "             186,  18264,    297,    678,     70, 144732,    111,  60091,   8783,\n",
       "           25251, 126416,  16227,  38627,      5,      2,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,  33086,     41,   1294,     12,  52231,    509,     70, 169209,\n",
       "          227204,     23, 129264,  84458,    100,     70,    345,      5,    294,\n",
       "               5,  16839,      7,   1295,  72392,     47, 140429,     32,      2,\n",
       "               2,  57212,    429,     12,  24447,  49456,  62952,    390, 172852,\n",
       "           69924, 227204,     71,    390,   3650, 143161,  96090,    100,  23991,\n",
       "            6602, 140429,   4743,     47,  92319,  85168,   5180,   8108, 152838,\n",
       "               4,  20697,  10323,  24365,  69924,      4,    136,  65572,     23,\n",
       "           27747,   3984,  13566,   1295,  18264, 142901,   4295,      5,      2,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,  33086,     41,   1294,     12,  52231,    509,     70, 169209,\n",
       "          227204,     23, 129264,  84458,    100,     70,    345,      5,    294,\n",
       "               5,  16839,      7,   1295,  72392,     47, 140429,     32,      2,\n",
       "               2,  57212,    429,     12, 225221, 177109,   8804,     25,      7,\n",
       "            3622, 146295,      7,  24124,     10, 144530,  51312,   1295,  72392,\n",
       "              47, 140429,      5,      2,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " 'teacher_scores': None}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47183bce-3be8-4b99-9eed-8e7c492a6ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RerankerCollator(tokenizer=XLMRobertaTokenizerFast(name_or_path='BAAI/bge-reranker-v2-m3', vocab_size=250002, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt', query_max_len=32, passage_max_len=128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5687d2e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RerankerCollator(tokenizer=XLMRobertaTokenizerFast(name_or_path='BAAI/bge-reranker-v2-m3', vocab_size=250002, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t250001: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt', query_max_len=32, passage_max_len=128)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e86a7f5-d2ef-4397-b990-f3f0cacd48a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AbsRerankerTrainingArguments(output_dir='./results', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, eval_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=5e-05, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=1000, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, lr_scheduler_kwargs={}, warmup_ratio=0.1, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./logs', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=25, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=100, save_total_limit=2, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='./results', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, tp_size=0, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, eval_use_gather_object=False, average_tokens_across_devices=False, sub_batch_size=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70103aa2-5fd5-423e-922b-fac05e0723db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2035/2691233529.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyRerankerTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyRerankerTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Add this before creating the trainer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Add a dummy train method to AdamW\n",
    "if not hasattr(AdamW, 'train'):\n",
    "    AdamW.train = lambda self: None\n",
    "\n",
    "# Then create your trainer as normal\n",
    "trainer = EncoderOnlyRerankerTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f554eff-d6be-4995-b13f-38635e927b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c0ec409-5374-4ef1-a609-9226282c2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a patched trainer class\n",
    "# class PatchedTrainer(EncoderOnlyRerankerTrainer):\n",
    "#     def training_step(self, model, inputs, num_items_in_batch):\n",
    "#         model.train()\n",
    "#         # Skip calling optimizer.train()\n",
    "        \n",
    "#         inputs = self._prepare_inputs(inputs)\n",
    "#         with self.compute_loss_context_manager():\n",
    "#             loss = self.compute_loss(model, inputs)\n",
    "            \n",
    "#         if self.args.gradient_accumulation_steps > 1:\n",
    "#             loss = loss / self.args.gradient_accumulation_steps\n",
    "            \n",
    "#         loss.backward()\n",
    "#         return loss.detach()\n",
    "\n",
    "# # Then use this trainer instead\n",
    "# trainer = PatchedTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921f09b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='497' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 497/1000 11:34 < 11:45, 0.71 it/s, Epoch 0.63/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.041900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d1a52-4987-4534-b91b-2d5adc577b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model onto disk\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501405b-1362-432a-9588-74bb9b1edb94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74f5fec-be8d-4f90-aa76-81d60caf0870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed93f8-cfde-4377-ad98-293d42235a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c65f4a-4529-4758-b688-736b0f5d84b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ddcf5-1e97-495f-b0d9-9b4f87466c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6214cd-883a-48af-80b6-b86db8e7ebc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14997d87-97f8-476f-bca1-693fdc866108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
