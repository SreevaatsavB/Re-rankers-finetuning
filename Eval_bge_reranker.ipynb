{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01efcdee-74e8-4948-a32b-fc85874af265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install pytrec_eval faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caae933-b83d-4136-93da-9e8f6777b621",
   "metadata": {},
   "source": [
    "### Evaluate BGE reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008dceec-f832-4d08-9785-beedaa84fb01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from FlagEmbedding.abc.evaluation.utils import evaluate_metrics, evaluate_mrr\n",
    "from FlagEmbedding import FlagModel, FlagReranker\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3916210f-25f7-4733-8ffd-7fb6491c4097",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd54b92d-bece-479f-9726-3cda82b8bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding.abc.evaluation.utils import evaluate_metrics, evaluate_mrr\n",
    "from FlagEmbedding import FlagModel, FlagReranker\n",
    "\n",
    "\n",
    "raw_model_id = \"BAAI/bge-base-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0b8f6f-b42f-4314-8b77-6a7b8d591f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73136f53e784f22937bc08314d4dcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f866b28b8a4018ac93d6ec2fe5304b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b58bcfbc88a4bc38472ad32069ad37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe04b60ab1b4f76aa4b586a4f3b0ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d81a5efbd024077b03ab3fed38e71ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324e4a789c2841ab86dcc4b234a0d43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at BAAI/bge-base-en-v1.5 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "raw_model = FlagReranker(raw_model_id, use_fp16=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f6ca9-902d-4f67-b8ec-ba526a40129b",
   "metadata": {},
   "source": [
    "### Load testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ea1d1b-4cd1-4757-8403-90a09dbe8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_data = []\n",
    "with open(\"ft_data/testing_data.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        test_data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831c6abc-5347-4490-b235-bd85b2d6efd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07252aa-9933-482e-a0a4-32e394da60a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How does Starbucks recognize the interest and penalties related to income tax matters on their financial statements?',\n",
       " 'pos': ['Starbucks recognizes interest and penalties related to income tax matters in income tax expense on our consolidated statements of earnings. Accrued interest and penalties are included within the related tax balances on our consolidated balance sheets.'],\n",
       " 'neg': ['The Australian Securities and Investments Commission regulates corporations and has authority to investigate, prosecute, ban individuals and to seek civil penalties.',\n",
       "  'Item 3 of the Annual Report on Form 10-K connects to information about legal proceedings by referring to Note 14 in the Notes to the Consolidated Financial Statements, included in Item 8.',\n",
       "  'The accumulated benefit obligation for the USRIP and Supplemental Retirement Plans was $466.1 million at December 31, 2023, and was $500.6 million at December 31, 2022.',\n",
       "  'Operating activities | $ | 2,296,164 |',\n",
       "  'Net cash provided by operating activities decreased by $349 million from $1,899 million in 2022 to $1,550 million in 2023.',\n",
       "  'Memberships are assigned to territories based on the geographic location used at time of sign-up as determined by the Companyâ€™s internal systems, which utilize industry standard geo-location technology.',\n",
       "  'Net cash used in financing activities was $656.5 million in the year ended December 31, 2023. This was primarily attributable to stock repurchases of $577.0 million and, to a lesser extent, payment of tax obligations on vested equity awards of $83.4 million.',\n",
       "  'The resulting cost is recognized over the period during which an employee is required to provide service in exchange for the awards, usually the vesting period, which is generally four years for stock options and RSUs.',\n",
       "  'FDA approved a label update to Yescarta to include overall survival data from the Phase 3 ZUMA-7 study.']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e457e2-a248-483d-9c1b-13f6091bf07b",
   "metadata": {},
   "source": [
    "### Create data for evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4871b5f2-d16e-43b6-983a-d254322c3664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries: 700\n",
      "Total negative examples: 6300\n",
      "Average negative examples per query: 9.00\n",
      "\n",
      "Sample entry:\n",
      "Query: How does Starbucks recognize the interest and penalties related to income tax matters on their financial statements?\n",
      "Positive document: Starbucks recognizes interest and penalties related to income tax matters in income tax expense on o...\n",
      "Number of negative documents: 9\n",
      "First negative document: The Australian Securities and Investments Commission regulates corporations and has authority to inv...\n",
      "\n",
      "Training data:\n",
      "Total examples: 7000\n",
      "Positive examples: 700\n",
      "Negative examples: 6300\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for item in test_data:\n",
    "    query_text = item[\"query\"]\n",
    "    positive_docs = item[\"pos\"]\n",
    "    negative_docs = item[\"neg\"]\n",
    "    \n",
    "    # For each positive document, create an entry\n",
    "    for pos_doc_text in positive_docs:\n",
    "        entry = {\n",
    "            \"query\": query_text,\n",
    "            \"pos\": pos_doc_text,\n",
    "            \"neg\": negative_docs\n",
    "        }\n",
    "        dataset.append(entry)\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total queries: {len(df)}\")\n",
    "print(f\"Total negative examples: {sum(len(row['neg']) for _, row in df.iterrows())}\")\n",
    "print(f\"Average negative examples per query: {sum(len(row['neg']) for _, row in df.iterrows()) / len(df):.2f}\")\n",
    "\n",
    "print(\"\\nSample entry:\")\n",
    "sample_entry = dataset[0]\n",
    "print(f\"Query: {sample_entry['query']}\")\n",
    "print(f\"Positive document: {sample_entry['pos'][:100]}...\")\n",
    "print(f\"Number of negative documents: {len(sample_entry['neg'])}\")\n",
    "print(f\"First negative document: {sample_entry['neg'][0][:100]}...\")\n",
    "\n",
    "# Convert to training format\n",
    "training_data = []\n",
    "for entry in dataset:\n",
    "    query = entry[\"query\"]\n",
    "    pos_doc = entry[\"pos\"]\n",
    "    \n",
    "    # Add positive example\n",
    "    training_data.append({\n",
    "        \"query\": query,\n",
    "        \"document\": pos_doc,\n",
    "        \"label\": 1\n",
    "    })\n",
    "    \n",
    "    # Add negative examples\n",
    "    for neg_doc in entry[\"neg\"]:\n",
    "        training_data.append({\n",
    "            \"query\": query,\n",
    "            \"document\": neg_doc,\n",
    "            \"label\": 0\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "training_df = pd.DataFrame(training_data)\n",
    "\n",
    "# Display training data statistics\n",
    "print(\"\\nTraining data:\")\n",
    "print(f\"Total examples: {len(training_df)}\")\n",
    "print(f\"Positive examples: {len(training_df[training_df['label'] == 1])}\")\n",
    "print(f\"Negative examples: {len(training_df[training_df['label'] == 0])}\")\n",
    "\n",
    "\n",
    "# Optional: Save the dataset to files\n",
    "# df.to_json(\"structured_dataset.json\", orient=\"records\")\n",
    "# training_df.to_csv(\"training_dataset.csv\", index=False)\n",
    "# training_df.to_json(\"training_dataset.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6282852-092d-4c98-b0fb-8fff24bda8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f360706f-2ed0-452f-88ba-a54afa227e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How does Starbucks recognize the interest and penalties related to income tax matters on their financial statements?',\n",
       " 'pos': 'Starbucks recognizes interest and penalties related to income tax matters in income tax expense on our consolidated statements of earnings. Accrued interest and penalties are included within the related tax balances on our consolidated balance sheets.',\n",
       " 'neg': ['The Australian Securities and Investments Commission regulates corporations and has authority to investigate, prosecute, ban individuals and to seek civil penalties.',\n",
       "  'Item 3 of the Annual Report on Form 10-K connects to information about legal proceedings by referring to Note 14 in the Notes to the Consolidated Financial Statements, included in Item 8.',\n",
       "  'The accumulated benefit obligation for the USRIP and Supplemental Retirement Plans was $466.1 million at December 31, 2023, and was $500.6 million at December 31, 2022.',\n",
       "  'Operating activities | $ | 2,296,164 |',\n",
       "  'Net cash provided by operating activities decreased by $349 million from $1,899 million in 2022 to $1,550 million in 2023.',\n",
       "  'Memberships are assigned to territories based on the geographic location used at time of sign-up as determined by the Companyâ€™s internal systems, which utilize industry standard geo-location technology.',\n",
       "  'Net cash used in financing activities was $656.5 million in the year ended December 31, 2023. This was primarily attributable to stock repurchases of $577.0 million and, to a lesser extent, payment of tax obligations on vested equity awards of $83.4 million.',\n",
       "  'The resulting cost is recognized over the period during which an employee is required to provide service in exchange for the awards, usually the vesting period, which is generally four years for stock options and RSUs.',\n",
       "  'FDA approved a label update to Yescarta to include overall survival data from the Phase 3 ZUMA-7 study.']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb23b5-bd99-4935-8dd6-46245fdd8f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33a140a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56803d28-602c-4762-b53c-52f4f899e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def rerank_predictions(query, docs, model=None):\n",
    "    \"\"\"\n",
    "    Rerank documents based on their relevance to the query using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query text\n",
    "        docs (list): List of document texts to rerank\n",
    "        model: The model to use for scoring. If None, uses the default model.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (document, score, rank) tuples sorted by score in descending order\n",
    "    \"\"\"\n",
    "    # If no model is provided, use the default model\n",
    "    if model is None:\n",
    "        # You would need to define or import your model here\n",
    "        raise ValueError(\"Model must be provided\")\n",
    "    \n",
    "    # Create pairs of query and document for scoring\n",
    "    pairs = [[query, doc] for doc in docs]\n",
    "    \n",
    "    # Compute scores for each query-document pair\n",
    "    scores = model.compute_score(pairs)\n",
    "    \n",
    "    # Combine documents with their scores\n",
    "    doc_scores = list(zip(docs, scores))\n",
    "    \n",
    "    # Sort by score in descending order (highest score first)\n",
    "    ranked_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Add rank information\n",
    "    ranked_docs_with_rank = [(doc, score, i+1) for i, (doc, score) in enumerate(ranked_docs)]\n",
    "    \n",
    "    return ranked_docs_with_rank\n",
    "\n",
    "# Function to calculate NDCG\n",
    "def calculate_ndcg(relevance_scores, k=None):\n",
    "    \"\"\"\n",
    "    Calculate NDCG (Normalized Discounted Cumulative Gain) at k.\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores (list): List of relevance scores (1 for relevant, 0 for non-relevant)\n",
    "        k (int, optional): Calculate NDCG@k. If None, use all scores.\n",
    "    \n",
    "    Returns:\n",
    "        float: NDCG value\n",
    "    \"\"\"\n",
    "    if not relevance_scores:\n",
    "        return 0.0\n",
    "    \n",
    "    if k is not None:\n",
    "        relevance_scores = relevance_scores[:k]\n",
    "    \n",
    "    # Calculate DCG (Discounted Cumulative Gain)\n",
    "    dcg = 0.0\n",
    "    for i, rel in enumerate(relevance_scores):\n",
    "        # Using log base 2 as is standard in NDCG\n",
    "        dcg += (2**rel - 1) / np.log2(i + 2)  # +2 because i is 0-indexed and log(1) is 0\n",
    "    \n",
    "    # Calculate ideal DCG (IDCG)\n",
    "    ideal_relevance = sorted(relevance_scores, reverse=True)\n",
    "    idcg = 0.0\n",
    "    for i, rel in enumerate(ideal_relevance):\n",
    "        idcg += (2**rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Calculate NDCG\n",
    "    if idcg > 0:\n",
    "        return dcg / idcg\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6285d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(eval_dataset, model, shuffle_docs=True):\n",
    "    \"\"\"\n",
    "    Evaluate a reranking model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        eval_dataset: List of dictionaries with 'query', 'pos', and 'neg' keys\n",
    "        model: The model to evaluate\n",
    "        shuffle_docs: Whether to shuffle documents to avoid position bias\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all evaluation metrics\n",
    "    \"\"\"\n",
    "    # Eval metrics \n",
    "    correct_at_1 = 0\n",
    "    mrr_sum = 0  # Mean Reciprocal Rank\n",
    "    ndcg_sum = 0  # NDCG\n",
    "    ndcg_at_3_sum = 0  # NDCG@3\n",
    "    ndcg_at_5_sum = 0  # NDCG@5\n",
    "\n",
    "    print(\"Evaluating model on dataset...\")\n",
    "    for entry in tqdm(eval_dataset, desc=\"Evaluating queries\"):\n",
    "        query = entry[\"query\"]\n",
    "        positive_doc = entry[\"pos\"]\n",
    "        negative_docs = entry[\"neg\"]\n",
    "        \n",
    "        # Combine positive and negative documents\n",
    "        all_docs = [positive_doc] + negative_docs\n",
    "        \n",
    "        # Shuffle documents to avoid position bias\n",
    "        if shuffle_docs:\n",
    "            random.shuffle(all_docs)\n",
    "        \n",
    "        # Rerank the documents\n",
    "        ranked_results = rerank_predictions(query, all_docs, model)\n",
    "        \n",
    "        # Find the rank of the positive document and create relevance list\n",
    "        positive_rank = None\n",
    "        relevance_scores = []\n",
    "        \n",
    "        for doc, score, rank in ranked_results:\n",
    "            # 1 for relevant (positive) document, 0 for non-relevant\n",
    "            relevance = 1 if doc == positive_doc else 0\n",
    "            relevance_scores.append(relevance)\n",
    "            \n",
    "            if doc == positive_doc:\n",
    "                positive_rank = rank\n",
    "        \n",
    "        # Update metrics\n",
    "        if positive_rank == 1:\n",
    "            correct_at_1 += 1\n",
    "        \n",
    "        mrr_sum += 1.0 / positive_rank if positive_rank else 0\n",
    "        \n",
    "        # Calculate NDCG metrics\n",
    "        ndcg = calculate_ndcg(relevance_scores)\n",
    "        ndcg_at_3 = calculate_ndcg(relevance_scores, k=3)\n",
    "        ndcg_at_5 = calculate_ndcg(relevance_scores, k=5)\n",
    "        \n",
    "        ndcg_sum += ndcg\n",
    "        ndcg_at_3_sum += ndcg_at_3\n",
    "        ndcg_at_5_sum += ndcg_at_5\n",
    "\n",
    "    # Calculate final metrics\n",
    "    total_queries = len(eval_dataset)\n",
    "    accuracy_at_1 = correct_at_1 / total_queries if total_queries > 0 else 0\n",
    "    mrr = mrr_sum / total_queries if total_queries > 0 else 0\n",
    "    ndcg_avg = ndcg_sum / total_queries if total_queries > 0 else 0\n",
    "    ndcg_at_3_avg = ndcg_at_3_sum / total_queries if total_queries > 0 else 0\n",
    "    ndcg_at_5_avg = ndcg_at_5_sum / total_queries if total_queries > 0 else 0\n",
    "\n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        \"total_queries\": total_queries,\n",
    "        \"accuracy_at_1\": accuracy_at_1,\n",
    "        \"mrr\": mrr,\n",
    "        \"ndcg\": ndcg_avg,\n",
    "        \"ndcg_at_3\": ndcg_at_3_avg,\n",
    "        \"ndcg_at_5\": ndcg_at_5_avg\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Total queries evaluated: {results['total_queries']}\")\n",
    "    print(f\"Accuracy@1: {results['accuracy_at_1']:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR): {results['mrr']:.4f}\")\n",
    "    print(f\"NDCG: {results['ndcg']:.4f}\")\n",
    "    print(f\"NDCG@3: {results['ndcg_at_3']:.4f}\")\n",
    "    print(f\"NDCG@5: {results['ndcg_at_5']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa5405a0-3a3b-4160-8bf3-536a5a946751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 700/700 [00:53<00:00, 12.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Total queries evaluated: 700\n",
      "Accuracy@1: 0.0514\n",
      "Mean Reciprocal Rank (MRR): 0.2439\n",
      "NDCG: 0.4162\n",
      "NDCG@3: 0.1445\n",
      "NDCG@5: 0.2413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(eval_dataset, raw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "657754ed-c271-4b12-af99-71a1966b0fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_queries': 700,\n",
       " 'accuracy_at_1': 0.05142857142857143,\n",
       " 'mrr': 0.24392687074829883,\n",
       " 'ndcg': np.float64(0.4161641625871046),\n",
       " 'ndcg_at_3': np.float64(0.144539167760206),\n",
       " 'ndcg_at_5': np.float64(0.24128697642883729)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bb03d-e84f-46de-a5f3-59d5123cea45",
   "metadata": {},
   "source": [
    "### Eval on finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42e43261",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_path = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cf8fae2-3b85-4340-8cc3-aaa7339cd62a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ft_model =  FlagReranker(ft_model_path, use_fp16=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1b658cc-1a98-4833-ae42-cb0198d63213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlagEmbedding.inference.reranker.encoder_only.base.BaseReranker at 0x784aba9414b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6807d92a-29a0-40f2-b361-3ff66323199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:   0%|          | 0/700 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Evaluating queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 700/700 [00:27<00:00, 25.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Total queries evaluated: 700\n",
      "Accuracy@1: 0.9943\n",
      "Mean Reciprocal Rank (MRR): 0.9971\n",
      "NDCG: 0.9979\n",
      "NDCG@3: 0.9979\n",
      "NDCG@5: 0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(eval_dataset, ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56db5c0-4a77-462f-8061-b6a034e42369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
