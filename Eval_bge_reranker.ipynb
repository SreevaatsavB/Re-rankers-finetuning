{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01efcdee-74e8-4948-a32b-fc85874af265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caae933-b83d-4136-93da-9e8f6777b621",
   "metadata": {},
   "source": [
    "### Evaluate BGE reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "008dceec-f832-4d08-9785-beedaa84fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding.abc.evaluation.utils import evaluate_metrics, evaluate_mrr\n",
    "from FlagEmbedding import FlagModel, FlagReranker\n",
    "\n",
    "k_values = [10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea003883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytrec_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3916210f-25f7-4733-8ffd-7fb6491c4097",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd54b92d-bece-479f-9726-3cda82b8bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding.abc.evaluation.utils import evaluate_metrics, evaluate_mrr\n",
    "from FlagEmbedding import FlagModel, FlagReranker\n",
    "\n",
    "\n",
    "raw_model = \"BAAI/bge-reranker-v2-m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0b8f6f-b42f-4314-8b77-6a7b8d591f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model = FlagReranker(\"BAAI/bge-reranker-v2-m3\", use_fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d6b54bc-a74c-47da-94c7-7051afc5fed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.781817436218262, -10.811635971069336, -11.013592720031738, 8.00526237487793]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [\n",
    "    [\"What is the capital of France?\", \"Paris is the capital of France.\"],\n",
    "    [\"What is the capital of France?\", \"The population of China is over 1.4 billion people.\"],\n",
    "    [\"What is the population of China?\", \"Paris is the capital of France.\"],\n",
    "    [\"What is the population of China?\", \"The population of China is over 1.4 billion people.\"]\n",
    "]\n",
    "\n",
    "scores = raw_model.compute_score(pairs)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3880c605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1289',\n",
       " 'text': 'How does Starbucks recognize the interest and penalties related to income tax matters on their financial statements?'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2340199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_query_passage_pairs(queries, corpus, qrels):\n",
    "    pairs = []\n",
    "    \n",
    "    for query in queries:\n",
    "        for passage in corpus:\n",
    "            pairs.append([query, passage])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b24649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "queries = load_dataset(\"json\", data_files=\"ft_data/test_queries.jsonl\")[\"train\"]\n",
    "corpus = load_dataset(\"json\", data_files=\"ft_data/corpus.jsonl\")[\"train\"]\n",
    "qrels = load_dataset(\"json\", data_files=\"ft_data/test_qrels.jsonl\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e755c2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 7000, 700)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries), len(corpus), len(qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcda3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = queries[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47302342-40f4-4120-99b9-c1490c36effc",
   "metadata": {},
   "source": [
    "### Data preperation for evals :- query, pos, neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9034c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries: 700\n",
      "Total negative examples: 3500\n",
      "Average negative examples per query: 5.00\n",
      "\n",
      "Sample entry:\n",
      "Query ID: 1289\n",
      "Query text: How does Starbucks recognize the interest and penalties related to income tax matters on their financial statements?\n",
      "Positive document ID: 1289\n",
      "Positive document text: Starbucks recognizes interest and penalties related to income tax matters in income tax expense on o...\n",
      "Number of negative documents: 5\n",
      "First negative document ID: 3135\n",
      "First negative document text: Consumer Banking Results Net income for Consumer Banking decreased $923 million to $11.6 billion due...\n",
      "\n",
      "Training data:\n",
      "Total examples: 4200\n",
      "Positive examples: 700\n",
      "Negative examples: 3500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does Starbucks recognize the interest and ...</td>\n",
       "      <td>Starbucks recognizes interest and penalties re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does Starbucks recognize the interest and ...</td>\n",
       "      <td>Consumer Banking Results Net income for Consum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does Starbucks recognize the interest and ...</td>\n",
       "      <td>The report on the Consolidated Financial State...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does Starbucks recognize the interest and ...</td>\n",
       "      <td>Iron Mountain expects to incur approximately $...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does Starbucks recognize the interest and ...</td>\n",
       "      <td>Gross profit margin for the Dollar Tree segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  How does Starbucks recognize the interest and ...   \n",
       "1  How does Starbucks recognize the interest and ...   \n",
       "2  How does Starbucks recognize the interest and ...   \n",
       "3  How does Starbucks recognize the interest and ...   \n",
       "4  How does Starbucks recognize the interest and ...   \n",
       "\n",
       "                                            document  label  \n",
       "0  Starbucks recognizes interest and penalties re...      1  \n",
       "1  Consumer Banking Results Net income for Consum...      0  \n",
       "2  The report on the Consolidated Financial State...      0  \n",
       "3  Iron Mountain expects to incur approximately $...      0  \n",
       "4  Gross profit margin for the Dollar Tree segmen...      0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_dict = {q[\"id\"]: q[\"text\"] for q in queries}\n",
    "corpus_dict = {d[\"id\"]: d[\"text\"][0] if isinstance(d[\"text\"], list) else d[\"text\"] for d in corpus}\n",
    "\n",
    "\n",
    "# Create a structured dataset with positive and negative examples for each query\n",
    "dataset = []\n",
    "\n",
    "# For each query in qrels\n",
    "for qrel in qrels:\n",
    "    qid = qrel[\"qid\"]\n",
    "    pos_docid = qrel[\"docid\"]\n",
    "    \n",
    "    if qid in queries_dict and pos_docid in corpus_dict:\n",
    "        query_text = queries_dict[qid]\n",
    "        pos_doc_text = corpus_dict[pos_docid]\n",
    "        \n",
    "        # Get all document IDs that are not positive matches for this query\n",
    "        negative_docids = [doc_id for doc_id in corpus_dict.keys() if doc_id != pos_docid]\n",
    "        \n",
    "        # Sample negative examples for this query (adjust the number as needed)\n",
    "        num_negatives = min(5, len(negative_docids))  # 5 negatives per query\n",
    "        if negative_docids and num_negatives > 0:\n",
    "            sampled_negative_docids = random.sample(negative_docids, num_negatives)\n",
    "            \n",
    "            # Create an entry with the query, positive document, and negative documents\n",
    "            entry = {\n",
    "                \"query_id\": qid,\n",
    "                \"query_text\": query_text,\n",
    "                \"positive_doc_id\": pos_docid,\n",
    "                \"positive_doc_text\": pos_doc_text,\n",
    "                \"negative_docs\": [\n",
    "                    {\n",
    "                        \"doc_id\": neg_docid,\n",
    "                        \"doc_text\": corpus_dict[neg_docid]\n",
    "                    } for neg_docid in sampled_negative_docids\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            dataset.append(entry)\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "print(f\"Total queries: {len(df)}\")\n",
    "print(f\"Total negative examples: {sum(len(row['negative_docs']) for _, row in df.iterrows())}\")\n",
    "print(f\"Average negative examples per query: {sum(len(row['negative_docs']) for _, row in df.iterrows()) / len(df):.2f}\")\n",
    "\n",
    "print(\"\\nSample entry:\")\n",
    "sample_entry = dataset[0]\n",
    "print(f\"Query ID: {sample_entry['query_id']}\")\n",
    "print(f\"Query text: {sample_entry['query_text']}\")\n",
    "print(f\"Positive document ID: {sample_entry['positive_doc_id']}\")\n",
    "print(f\"Positive document text: {sample_entry['positive_doc_text'][:100]}...\")\n",
    "print(f\"Number of negative documents: {len(sample_entry['negative_docs'])}\")\n",
    "print(f\"First negative document ID: {sample_entry['negative_docs'][0]['doc_id']}\")\n",
    "print(f\"First negative document text: {sample_entry['negative_docs'][0]['doc_text'][:100]}...\")\n",
    "\n",
    "# Optional: Convert to a format suitable for training\n",
    "training_data = []\n",
    "for entry in dataset:\n",
    "    query = entry[\"query_text\"]\n",
    "    pos_doc = entry[\"positive_doc_text\"]\n",
    "    \n",
    "    # Add positive example\n",
    "    training_data.append({\n",
    "        \"query\": query,\n",
    "        \"document\": pos_doc,\n",
    "        \"label\": 1\n",
    "    })\n",
    "    \n",
    "    # Add negative examples\n",
    "    for neg_doc in entry[\"negative_docs\"]:\n",
    "        training_data.append({\n",
    "            \"query\": query,\n",
    "            \"document\": neg_doc[\"doc_text\"],\n",
    "            \"label\": 0\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "training_df = pd.DataFrame(training_data)\n",
    "\n",
    "# Display training data statistics\n",
    "print(\"\\nTraining data:\")\n",
    "print(f\"Total examples: {len(training_df)}\")\n",
    "print(f\"Positive examples: {len(training_df[training_df['label'] == 1])}\")\n",
    "print(f\"Negative examples: {len(training_df[training_df['label'] == 0])}\")\n",
    "\n",
    "# Preview training data\n",
    "training_df.head()\n",
    "\n",
    "# Optional: Save the dataset to a file\n",
    "# df.to_json(\"structured_dataset.json\", orient=\"records\")\n",
    "# training_df.to_csv(\"training_dataset.csv\", index=False)\n",
    "# training_df.to_json(\"training_dataset.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b553f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.to_json(\"structured_dataset.json\", orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b33ed9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33a140a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = dataset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56803d28-602c-4762-b53c-52f4f899e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def rerank_predictions(query, docs, model=None):\n",
    "    \"\"\"\n",
    "    Rerank documents based on their relevance to the query using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query text\n",
    "        docs (list): List of document texts to rerank\n",
    "        model: The model to use for scoring. If None, uses the default model.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (document, score, rank) tuples sorted by score in descending order\n",
    "    \"\"\"\n",
    "    # If no model is provided, use the default model\n",
    "    if model is None:\n",
    "        # You would need to define or import your model here\n",
    "        raise ValueError(\"Model must be provided\")\n",
    "    \n",
    "    # Create pairs of query and document for scoring\n",
    "    pairs = [[query, doc] for doc in docs]\n",
    "    \n",
    "    # Compute scores for each query-document pair\n",
    "    scores = model.compute_score(pairs)\n",
    "    \n",
    "    # Combine documents with their scores\n",
    "    doc_scores = list(zip(docs, scores))\n",
    "    \n",
    "    # Sort by score in descending order (highest score first)\n",
    "    ranked_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Add rank information\n",
    "    ranked_docs_with_rank = [(doc, score, i+1) for i, (doc, score) in enumerate(ranked_docs)]\n",
    "    \n",
    "    return ranked_docs_with_rank\n",
    "\n",
    "# Function to calculate NDCG\n",
    "def calculate_ndcg(relevance_scores, k=None):\n",
    "    \"\"\"\n",
    "    Calculate NDCG (Normalized Discounted Cumulative Gain) at k.\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores (list): List of relevance scores (1 for relevant, 0 for non-relevant)\n",
    "        k (int, optional): Calculate NDCG@k. If None, use all scores.\n",
    "    \n",
    "    Returns:\n",
    "        float: NDCG value\n",
    "    \"\"\"\n",
    "    if not relevance_scores:\n",
    "        return 0.0\n",
    "    \n",
    "    if k is not None:\n",
    "        relevance_scores = relevance_scores[:k]\n",
    "    \n",
    "    # Calculate DCG (Discounted Cumulative Gain)\n",
    "    dcg = 0.0\n",
    "    for i, rel in enumerate(relevance_scores):\n",
    "        # Using log base 2 as is standard in NDCG\n",
    "        dcg += (2**rel - 1) / np.log2(i + 2)  # +2 because i is 0-indexed and log(1) is 0\n",
    "    \n",
    "    # Calculate ideal DCG (IDCG)\n",
    "    ideal_relevance = sorted(relevance_scores, reverse=True)\n",
    "    idcg = 0.0\n",
    "    for i, rel in enumerate(ideal_relevance):\n",
    "        idcg += (2**rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Calculate NDCG\n",
    "    if idcg > 0:\n",
    "        return dcg / idcg\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6285d68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 100/100 [00:07<00:00, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Total queries evaluated: 100\n",
      "Accuracy@1: 0.9900\n",
      "Mean Reciprocal Rank (MRR): 0.9950\n",
      "NDCG: 0.9963\n",
      "NDCG@3: 0.9963\n",
      "NDCG@5: 0.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Eval metrics \n",
    "\n",
    "correct_at_1 = 0\n",
    "mrr_sum = 0  # Mean Reciprocal Rank\n",
    "ndcg_sum = 0  # NDCG\n",
    "ndcg_at_3_sum = 0  # NDCG@3\n",
    "ndcg_at_5_sum = 0  # NDCG@5\n",
    "\n",
    "\n",
    "\n",
    "print(\"Evaluating model on dataset...\")\n",
    "for entry in tqdm(eval_dataset, desc=\"Evaluating queries\"):\n",
    "    query = entry[\"query_text\"]\n",
    "    positive_doc = entry[\"positive_doc_text\"]\n",
    "    \n",
    "    # Combine positive and negative documents\n",
    "    all_docs = [positive_doc] + [neg_doc[\"doc_text\"] for neg_doc in entry[\"negative_docs\"]]\n",
    "    \n",
    "    # Shuffle documents to avoid position bias\n",
    "    random.shuffle(all_docs)\n",
    "    \n",
    "    # Rerank the documents\n",
    "    ranked_results = rerank_predictions(query, all_docs, raw_model)\n",
    "    \n",
    "    # Find the rank of the positive document and create relevance list\n",
    "    positive_rank = None\n",
    "    relevance_scores = []\n",
    "    \n",
    "    for doc, score, rank in ranked_results:\n",
    "        # 1 for relevant (positive) document, 0 for non-relevant\n",
    "        relevance = 1 if doc == positive_doc else 0\n",
    "        relevance_scores.append(relevance)\n",
    "        \n",
    "        if doc == positive_doc:\n",
    "            positive_rank = rank\n",
    "    \n",
    "    # Update metrics\n",
    "    if positive_rank == 1:\n",
    "        correct_at_1 += 1\n",
    "    \n",
    "    mrr_sum += 1.0 / positive_rank if positive_rank else 0\n",
    "    \n",
    "    # Calculate NDCG metrics\n",
    "    ndcg = calculate_ndcg(relevance_scores)\n",
    "    ndcg_at_3 = calculate_ndcg(relevance_scores, k=3)\n",
    "    ndcg_at_5 = calculate_ndcg(relevance_scores, k=5)\n",
    "    \n",
    "    ndcg_sum += ndcg\n",
    "    ndcg_at_3_sum += ndcg_at_3\n",
    "    ndcg_at_5_sum += ndcg_at_5\n",
    "\n",
    "# Calculate final metrics\n",
    "total_queries = len(eval_dataset)\n",
    "accuracy_at_1 = correct_at_1 / total_queries if total_queries > 0 else 0\n",
    "mrr = mrr_sum / total_queries if total_queries > 0 else 0\n",
    "ndcg_avg = ndcg_sum / total_queries if total_queries > 0 else 0\n",
    "ndcg_at_3_avg = ndcg_at_3_sum / total_queries if total_queries > 0 else 0\n",
    "ndcg_at_5_avg = ndcg_at_5_sum / total_queries if total_queries > 0 else 0\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Total queries evaluated: {total_queries}\")\n",
    "print(f\"Accuracy@1: {accuracy_at_1:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "print(f\"NDCG: {ndcg_avg:.4f}\")\n",
    "print(f\"NDCG@3: {ndcg_at_3_avg:.4f}\")\n",
    "print(f\"NDCG@5: {ndcg_at_5_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1483973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Example of reranking for a single query\n",
    "# if eval_dataset:\n",
    "#     sample_entry = eval_dataset[0]\n",
    "#     sample_query = sample_entry[\"query_text\"]\n",
    "#     sample_pos_doc = sample_entry[\"positive_doc_text\"]\n",
    "#     sample_neg_docs = [neg_doc[\"doc_text\"] for neg_doc in sample_entry[\"negative_docs\"]]\n",
    "    \n",
    "#     all_sample_docs = [sample_pos_doc] + sample_neg_docs\n",
    "    \n",
    "#     print(\"\\nExample reranking for query:\")\n",
    "#     print(f\"Query: {sample_query}\")\n",
    "    \n",
    "#     ranked_sample = rerank_predictions(sample_query, all_sample_docs, raw_model)\n",
    "    \n",
    "#     print(\"\\nRanked results:\")\n",
    "#     for doc, score, rank in ranked_sample:\n",
    "#         doc_type = \"POSITIVE\" if doc == sample_pos_doc else \"NEGATIVE\"\n",
    "#         print(f\"Rank {rank} | Score: {score:.4f} | Type: {doc_type}\")\n",
    "#         print(f\"Document: {doc[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bb03d-e84f-46de-a5f3-59d5123cea45",
   "metadata": {},
   "source": [
    "### Eval on finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42e43261",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_path = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cf8fae2-3b85-4340-8cc3-aaa7339cd62a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ft_model =  FlagReranker(ft_model_path, use_fp16=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1b658cc-1a98-4833-ae42-cb0198d63213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlagEmbedding.inference.reranker.encoder_only.base.BaseReranker at 0x76d1675fafb0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6807d92a-29a0-40f2-b361-3ff66323199a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries:   0%|          | 0/100 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Evaluating queries: 100%|██████████| 100/100 [00:21<00:00,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Total queries evaluated: 100\n",
      "Accuracy@1: 1.0000\n",
      "Mean Reciprocal Rank (MRR): 1.0000\n",
      "NDCG: 1.0000\n",
      "NDCG@3: 1.0000\n",
      "NDCG@5: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize evaluation metrics\n",
    "correct_at_1 = 0\n",
    "mrr_sum = 0  # Mean Reciprocal Rank\n",
    "ndcg_sum = 0  # NDCG\n",
    "ndcg_at_3_sum = 0  # NDCG@3\n",
    "ndcg_at_5_sum = 0  # NDCG@5\n",
    "\n",
    "# Evaluate each query in the dataset with progress bar\n",
    "print(\"Evaluating model on dataset...\")\n",
    "for entry in tqdm(eval_dataset, desc=\"Evaluating queries\"):\n",
    "    query = entry[\"query_text\"]\n",
    "    positive_doc = entry[\"positive_doc_text\"]\n",
    "    \n",
    "    # Combine positive and negative documents\n",
    "    all_docs = [positive_doc] + [neg_doc[\"doc_text\"] for neg_doc in entry[\"negative_docs\"]]\n",
    "    \n",
    "    # Shuffle documents to avoid position bias\n",
    "    random.shuffle(all_docs)\n",
    "    \n",
    "    # Rerank the documents\n",
    "    ranked_results = rerank_predictions(query, all_docs, ft_model)\n",
    "    \n",
    "    # Find the rank of the positive document and create relevance list\n",
    "    positive_rank = None\n",
    "    relevance_scores = []\n",
    "    \n",
    "    for doc, score, rank in ranked_results:\n",
    "        # 1 for relevant (positive) document, 0 for non-relevant\n",
    "        relevance = 1 if doc == positive_doc else 0\n",
    "        relevance_scores.append(relevance)\n",
    "        \n",
    "        if doc == positive_doc:\n",
    "            positive_rank = rank\n",
    "    \n",
    "    # Update metrics\n",
    "    if positive_rank == 1:\n",
    "        correct_at_1 += 1\n",
    "    \n",
    "    mrr_sum += 1.0 / positive_rank if positive_rank else 0\n",
    "    \n",
    "    # Calculate NDCG metrics\n",
    "    ndcg = calculate_ndcg(relevance_scores)\n",
    "    ndcg_at_3 = calculate_ndcg(relevance_scores, k=3)\n",
    "    ndcg_at_5 = calculate_ndcg(relevance_scores, k=5)\n",
    "    \n",
    "    ndcg_sum += ndcg\n",
    "    ndcg_at_3_sum += ndcg_at_3\n",
    "    ndcg_at_5_sum += ndcg_at_5\n",
    "\n",
    "# Calculate final metrics\n",
    "total_queries = len(eval_dataset)\n",
    "accuracy_at_1 = correct_at_1 / total_queries if total_queries > 0 else 0\n",
    "mrr = mrr_sum / total_queries if total_queries > 0 else 0\n",
    "ndcg_avg = ndcg_sum / total_queries if total_queries > 0 else 0\n",
    "ndcg_at_3_avg = ndcg_at_3_sum / total_queries if total_queries > 0 else 0\n",
    "ndcg_at_5_avg = ndcg_at_5_sum / total_queries if total_queries > 0 else 0\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Total queries evaluated: {total_queries}\")\n",
    "print(f\"Accuracy@1: {accuracy_at_1:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "print(f\"NDCG: {ndcg_avg:.4f}\")\n",
    "print(f\"NDCG@3: {ndcg_at_3_avg:.4f}\")\n",
    "print(f\"NDCG@5: {ndcg_at_5_avg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
